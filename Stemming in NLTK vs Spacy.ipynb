{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "650a8e32",
   "metadata": {},
   "source": [
    "stemming in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcf81de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer  = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60d01fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"eating\", \"eats\", \"eat\", \"ate\", \"adjustable\", \"rafting\", \"ability\", \"meeting\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "108d6d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating  |  eat\n",
      "eats  |  eat\n",
      "eat  |  eat\n",
      "ate  |  ate\n",
      "adjustable  |  adjust\n",
      "rafting  |  raft\n",
      "ability  |  abil\n",
      "meeting  |  meet\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word , \" | \" ,stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d0af6a",
   "metadata": {},
   "source": [
    "Lemmatization in spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eb2982c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "588fb831",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b58de3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = nlp(\"Mando talked for 3 hours although talking isn't his thing\")\n",
    "doc = nlp(\"eating eats eat ate adjustable rafting ability meeting better\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f901d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating  |  eat\n",
      "eats  |  eat\n",
      "eat  |  eat\n",
      "ate  |  eat\n",
      "adjustable  |  adjustable\n",
      "rafting  |  raft\n",
      "ability  |  ability\n",
      "meeting  |  meeting\n",
      "better  |  well\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token ,\" | \" , token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7df27e",
   "metadata": {},
   "source": [
    "Customizing lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "920c76b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c491fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = nlp.get_pipe(\"attribute_ruler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "906bd428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro  |  bro\n",
      ",  |  ,\n",
      "you  |  you\n",
      "wanna  |  wanna\n",
      "go  |  go\n",
      "?  |  ?\n",
      "Brah  |  Brah\n",
      ",  |  ,\n",
      "do  |  do\n",
      "n't  |  not\n",
      "say  |  say\n",
      "no  |  no\n",
      "!  |  !\n",
      "I  |  I\n",
      "am  |  be\n",
      "exhausted  |  exhaust\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Bro, you wanna go? Brah, don't say no! I am exhausted\")\n",
    "for token in doc:\n",
    "    print(token , \" | \" , token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bd17b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar.add([[{\"TEXT\":\"Bro\" }],[{\"TEXT\" : \"Brah\"}]],{\"LEMMA\" : \"Brother\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebe9fefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro  |  Brother  |  None\n",
      ",  |  ,  |  None\n",
      "you  |  you  |  None\n",
      "wanna  |  wanna  |  None\n",
      "go  |  go  |  None\n",
      "?  |  ?  |  None\n",
      "Brah  |  Brother  |  None\n",
      ",  |  ,  |  None\n",
      "do  |  do  |  None\n",
      "n't  |  not  |  None\n",
      "say  |  say  |  None\n",
      "no  |  no  |  None\n",
      "!  |  !  |  None\n",
      "I  |  I  |  None\n",
      "am  |  be  |  None\n",
      "exhausted  |  exhaust  |  None\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Bro, you wanna go? Brah, don't say no! I am exhausted\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token , \" | \" , token.lemma_, \" | \", spacy.explain(token.lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22899632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Brah"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "586d9116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Brother'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[6].lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "671ff710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro  |  NOUN  |  None\n",
      ",  |  PUNCT  |  None\n",
      "you  |  PRON  |  None\n",
      "wanna  |  AUX  |  None\n",
      "go  |  VERB  |  None\n",
      "?  |  PUNCT  |  None\n",
      "Brah  |  PROPN  |  None\n",
      ",  |  PUNCT  |  None\n",
      "do  |  AUX  |  None\n",
      "n't  |  PART  |  None\n",
      "say  |  VERB  |  None\n",
      "no  |  INTJ  |  None\n",
      "!  |  PUNCT  |  None\n",
      "I  |  PRON  |  None\n",
      "am  |  AUX  |  None\n",
      "exhausted  |  VERB  |  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vicky\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\glossary.py:19: UserWarning: [W118] Term '92' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
      "  warnings.warn(Warnings.W118.format(term=term))\n",
      "C:\\Users\\Vicky\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\glossary.py:19: UserWarning: [W118] Term '97' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
      "  warnings.warn(Warnings.W118.format(term=term))\n",
      "C:\\Users\\Vicky\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\glossary.py:19: UserWarning: [W118] Term '95' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
      "  warnings.warn(Warnings.W118.format(term=term))\n",
      "C:\\Users\\Vicky\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\glossary.py:19: UserWarning: [W118] Term '87' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
      "  warnings.warn(Warnings.W118.format(term=term))\n",
      "C:\\Users\\Vicky\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\glossary.py:19: UserWarning: [W118] Term '100' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
      "  warnings.warn(Warnings.W118.format(term=term))\n",
      "C:\\Users\\Vicky\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\glossary.py:19: UserWarning: [W118] Term '96' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
      "  warnings.warn(Warnings.W118.format(term=term))\n",
      "C:\\Users\\Vicky\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\glossary.py:19: UserWarning: [W118] Term '94' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
      "  warnings.warn(Warnings.W118.format(term=term))\n",
      "C:\\Users\\Vicky\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\glossary.py:19: UserWarning: [W118] Term '91' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
      "  warnings.warn(Warnings.W118.format(term=term))\n"
     ]
    }
   ],
   "source": [
    "#  POS\n",
    "\n",
    "doc = nlp(\"Bro, you wanna go? Brah, don't say no! I am exhausted\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token , \" | \" , token.pos_, \" | \", spacy.explain(token.pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fdd9fe",
   "metadata": {},
   "source": [
    "# EXCERISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3bb53e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d838a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_words = ['running', 'painting', 'walking', 'dressing', 'likely', 'children', 'whom', 'good', 'ate', 'fishing']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f72dc8",
   "metadata": {},
   "source": [
    " Stemming achieves the base word by removing the suffixes [ing, ly etc], so it successfully transform the words like 'painting', 'likely', 'fishing' and lemmatization fails for some words ending with suffixes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7006686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running  |  run\n",
      "painting  |  paint\n",
      "walking  |  walk\n",
      "dressing  |  dress\n",
      "likely  |  like\n",
      "children  |  children\n",
      "whom  |  whom\n",
      "good  |  good\n",
      "ate  |  ate\n",
      "fishing  |  fish\n"
     ]
    }
   ],
   "source": [
    "for words in lst_words:\n",
    "    print(words , \" | \" , stemmer.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afc19295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a43dc5",
   "metadata": {},
   "source": [
    " Lemmatization uses the dictionary meanings while converting to the base form, so words like 'children' and 'ate' are successfully transformed and stemming fails here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7acad32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running  |  run\n",
      "painting  |  paint\n",
      "walking  |  walk\n",
      "dressing  |  dress\n",
      "likely  |  likely\n",
      "children  |  child\n",
      "whom  |  whom\n",
      "good  |  good\n",
      "ate  |  eat\n",
      "fishing  |  fishing\n"
     ]
    }
   ],
   "source": [
    "docs = nlp(\"running painting walking dressing likely children whom good ate fishing\")\n",
    "\n",
    "for token in docs:\n",
    "    print(token , \" | \" , token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c8059",
   "metadata": {},
   "source": [
    "Excerise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7462933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Natural language processing (NLP) refers to the branch of computer science—and more specifically, the branch of artificial intelligence or AI—concerned with giving computers the ability to understand text and spoken words in much the same way human beings can.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54edb76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natur languag process ( nlp ) refer to the branch of comput science—and more specif , the branch of artifici intellig or ai—concern with give comput the abil to understand text and spoken word in much the same way human be can .\n"
     ]
    }
   ],
   "source": [
    "all_word_tokens = nltk.word_tokenize(text)\n",
    "\n",
    "all_base_words = []\n",
    "\n",
    "for token in all_word_tokens:\n",
    "    base_form = stemmer.stem(token)\n",
    "    all_base_words.append(base_form)\n",
    "    \n",
    "\n",
    "final_base_text = ' '.join(all_base_words)\n",
    "print(final_base_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a48ef80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural language processing ( NLP ) refer to the branch of computer science — and more specifically , the branch of artificial intelligence or AI — concern with give computer the ability to understand text and spoken word in much the same way human being can . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc  = nlp(text)\n",
    "all_base_words = []\n",
    "\n",
    "for token in doc:\n",
    "    base_word = token.lemma_\n",
    "    all_base_words.append(base_word)\n",
    "    \n",
    "final_base_text = ' '.join(all_base_words)\n",
    "print(final_base_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d36ab03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bd0657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9646a656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c96736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
